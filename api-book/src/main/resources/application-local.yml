langchain4j:
  ollama:
    chat-model:
      base-url: http://localhost:11434
      model-name: llama2
      temperature: 0.7